{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div id=\"my_id_menu_nb\">run previous cell, wait for 2 seconds</div>\n",
       "<script>\n",
       "function repeat_indent_string(n){\n",
       "    var a = \"\" ;\n",
       "    for ( ; n > 0 ; --n)\n",
       "        a += \"    \";\n",
       "    return a;\n",
       "}\n",
       "// look up into all sections and builds an automated menu //\n",
       "var update_menu_string = function(begin, lfirst, llast, sformat, send, keep_item, begin_format, end_format) {\n",
       "    var anchors = document.getElementsByClassName(\"section\");\n",
       "    if (anchors.length == 0) {\n",
       "        anchors = document.getElementsByClassName(\"text_cell_render rendered_html\");\n",
       "    }\n",
       "    var i,t;\n",
       "    var text_menu = begin;\n",
       "    var text_memo = \"<pre>\\nlength:\" + anchors.length + \"\\n\";\n",
       "    var ind = \"\";\n",
       "    var memo_level = 1;\n",
       "    var href;\n",
       "    var tags = [];\n",
       "    var main_item = 0;\n",
       "    var format_open = 0;\n",
       "    for (i = 0; i <= llast; i++)\n",
       "        tags.push(\"h\" + i);\n",
       "\n",
       "    for (i = 0; i < anchors.length; i++) {\n",
       "        text_memo += \"**\" + anchors[i].id + \"--\\n\";\n",
       "\n",
       "        var child = null;\n",
       "        for(t = 0; t < tags.length; t++) {\n",
       "            var r = anchors[i].getElementsByTagName(tags[t]);\n",
       "            if (r.length > 0) {\n",
       "child = r[0];\n",
       "break;\n",
       "            }\n",
       "        }\n",
       "        if (child == null) {\n",
       "            text_memo += \"null\\n\";\n",
       "            continue;\n",
       "        }\n",
       "        if (anchors[i].hasAttribute(\"id\")) {\n",
       "            // when converted in RST\n",
       "            href = anchors[i].id;\n",
       "            text_memo += \"#1-\" + href;\n",
       "            // passer à child suivant (le chercher)\n",
       "        }\n",
       "        else if (child.hasAttribute(\"id\")) {\n",
       "            // in a notebook\n",
       "            href = child.id;\n",
       "            text_memo += \"#2-\" + href;\n",
       "        }\n",
       "        else {\n",
       "            text_memo += \"#3-\" + \"*\" + \"\\n\";\n",
       "            continue;\n",
       "        }\n",
       "        var title = child.textContent;\n",
       "        var level = parseInt(child.tagName.substring(1,2));\n",
       "\n",
       "        text_memo += \"--\" + level + \"?\" + lfirst + \"--\" + title + \"\\n\";\n",
       "\n",
       "        if ((level < lfirst) || (level > llast)) {\n",
       "            continue ;\n",
       "        }\n",
       "        if (title.endsWith('¶')) {\n",
       "            title = title.substring(0,title.length-1).replace(\"<\", \"&lt;\")\n",
       "         .replace(\">\", \"&gt;\").replace(\"&\", \"&amp;\");\n",
       "        }\n",
       "        if (title.length == 0) {\n",
       "            continue;\n",
       "        }\n",
       "\n",
       "        while (level < memo_level) {\n",
       "            text_menu += end_format + \"</ul>\\n\";\n",
       "            format_open -= 1;\n",
       "            memo_level -= 1;\n",
       "        }\n",
       "        if (level == lfirst) {\n",
       "            main_item += 1;\n",
       "        }\n",
       "        if (keep_item != -1 && main_item != keep_item + 1) {\n",
       "            // alert(main_item + \" - \" + level + \" - \" + keep_item);\n",
       "            continue;\n",
       "        }\n",
       "        while (level > memo_level) {\n",
       "            text_menu += \"<ul>\\n\";\n",
       "            memo_level += 1;\n",
       "        }\n",
       "        text_menu += repeat_indent_string(level-2);\n",
       "        text_menu += begin_format + sformat.replace(\"__HREF__\", href).replace(\"__TITLE__\", title);\n",
       "        format_open += 1;\n",
       "    }\n",
       "    while (1 < memo_level) {\n",
       "        text_menu += end_format + \"</ul>\\n\";\n",
       "        memo_level -= 1;\n",
       "        format_open -= 1;\n",
       "    }\n",
       "    text_menu += send;\n",
       "    //text_menu += \"\\n\" + text_memo;\n",
       "\n",
       "    while (format_open > 0) {\n",
       "        text_menu += end_format;\n",
       "        format_open -= 1;\n",
       "    }\n",
       "    return text_menu;\n",
       "};\n",
       "var update_menu = function() {\n",
       "    var sbegin = \"\";\n",
       "    var sformat = '<a href=\"#__HREF__\">__TITLE__</a>';\n",
       "    var send = \"\";\n",
       "    var begin_format = '<li>';\n",
       "    var end_format = '</li>';\n",
       "    var keep_item = -1;\n",
       "    var text_menu = update_menu_string(sbegin, 2, 4, sformat, send, keep_item,\n",
       "       begin_format, end_format);\n",
       "    var menu = document.getElementById(\"my_id_menu_nb\");\n",
       "    menu.innerHTML=text_menu;\n",
       "};\n",
       "window.setTimeout(update_menu,2000);\n",
       "            </script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import grid2op\n",
    "\n",
    "res = None\n",
    "try:\n",
    "    from jyquickhelper import add_notebook_menu\n",
    "    res = add_notebook_menu()\n",
    "except ModuleNotFoundError:\n",
    "    print(\"Impossible to automatically add a menu / table of content to this notebook.\\nYou can download \\\"jyquickhelper\\\" package with: \\n\\\"pip install jyquickhelper\\\"\")\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\virtualenvironment\\L2PRN_grid2op\\lib\\site-packages\\grid2op\\MakeEnv\\Make.py:265: UserWarning: You are using a development environment. This environment is not intended for training agents. It might not be up to date and its primary use if for tests (hence the \"test=True\" you passed as argument). Use at your own risk.\n",
      "  warnings.warn(_MAKE_DEV_ENV_WARN)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from grid2op import make\n",
    "from grid2op.Agent import RandomAgent\n",
    "max_iter = 100\n",
    "train_iter = 500\n",
    "env_name = \"rte_case14_redisp\"\n",
    "env = make(env_name,test=True)\n",
    "env.seed(0)#这是为了保证环境中所有的随机源文件是可复制的\n",
    "my_agent = RandomAgent(env.action_space)\n",
    "my_agent.seed(0)#这是为了保证random agent所有的actions是一样的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "172\n",
      "47\n",
      "117\n"
     ]
    }
   ],
   "source": [
    "#从agent视角看到的action\n",
    "for el in range(3):\n",
    "    print(my_agent.my_act(None,None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This action will:\n",
      "\t - NOT change anything to the injections\n",
      "\t - NOT perform any redispatching action\n",
      "\t - NOT force any line status\n",
      "\t - NOT switch any line status\n",
      "\t - NOT switch anything in the topology\n",
      "\t - Set the bus of the following element:\n",
      "\t \t - assign bus 2 to line (extremity) 1 [on substation 4]\n",
      "\t \t - assign bus 2 to line (extremity) 9 [on substation 4]\n",
      "\t \t - assign bus 1 to line (extremity) 11 [on substation 4]\n",
      "\t \t - assign bus 2 to line (origin) 17 [on substation 4]\n",
      "\t \t - assign bus 2 to load 4 [on substation 4]\n",
      "This action will:\n",
      "\t - NOT change anything to the injections\n",
      "\t - NOT perform any redispatching action\n",
      "\t - NOT force any line status\n",
      "\t - NOT switch any line status\n",
      "\t - Change the bus of the following element:\n",
      "\t \t - switch bus of line (origin) 11 [on substation 3]\n",
      "\t \t - switch bus of line (origin) 15 [on substation 3]\n",
      "\t - NOT force any particular bus configuration\n",
      "This action will:\n",
      "\t - NOT change anything to the injections\n",
      "\t - NOT perform any redispatching action\n",
      "\t - NOT force any line status\n",
      "\t - NOT switch any line status\n",
      "\t - NOT switch anything in the topology\n",
      "\t - Set the bus of the following element:\n",
      "\t \t - assign bus 2 to line (origin) 2 [on substation 8]\n",
      "\t \t - assign bus 1 to line (origin) 3 [on substation 8]\n",
      "\t \t - assign bus 1 to line (extremity) 16 [on substation 8]\n",
      "\t \t - assign bus 1 to line (extremity) 19 [on substation 8]\n",
      "\t \t - assign bus 1 to load 6 [on substation 8]\n"
     ]
    }
   ],
   "source": [
    "#从电网角度看到的action\n",
    "for el in range(3):\n",
    "    print(my_agent.act(None,None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To install l2rpn_baselines, either uncomment the cell below, or type, in a command prompt:\n",
      "\tD:\\virtualenvironment\\L2PRN_grid2op\\python.exe -m pip install l2rpn_baselines\n"
     ]
    }
   ],
   "source": [
    "print(\"To install l2rpn_baselines, either uncomment the cell below, or type, in a command prompt:\\n{}\".format(\n",
    "    (\"\\t{} -m pip install l2rpn_baselines\".format(sys.executable))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf2.0 friendly\n",
    "import numpy as np\n",
    "import random\n",
    "import warnings\n",
    "import l2rpn_baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#converters可以将observation类自动转化为向量，从数值映射actions来完成之间定义的aciton对象\n",
    "#本质上，converter允许agent控制action space，这样一个agent就可以控制一个简单的、通用格式的action\n",
    "#并能完成从简单结构到复杂action/observation的映射\n",
    "from grid2op.Agent import AgentWithConverter\n",
    "class MyAgent(AgentWithConverter):\n",
    "    def __init__(self, action_space, action_space_converter=None):\n",
    "        super(MyAgent, self).__init__(action_space=action_space, action_space_converter=action_space_converter)\n",
    "        # for example you can define here all the actions you will consider\n",
    "        self.my_actions = [action_space(),\n",
    "                           action_space({\"redispatching\": [0,+1]}),\n",
    "                           action_space({\"set_line_status\": [(0,-1)]}),\n",
    "                           action_space({\"change_bus\": {\"lines_or_id\": [12]}}),\n",
    "                          ...\n",
    "                          ]\n",
    "        # or load them from a file for example...\n",
    "        # self.my_action = np.load(\"my_action_pre_selected.npy\")\n",
    "\n",
    "        # you can also in this agent load a neural network...\n",
    "        self.my_nn_model = model.load(\"my_saved_neural_network_weights.h5\")\n",
    "\n",
    "    def convert_obs(self, observation):\n",
    "        \"\"\"\n",
    "        This method is used to convert the observation, represented as a class Observation in input\n",
    "        into a \"transformed_observation\" that will be manipulated by the agent\n",
    "        An example here will transform the observation into a numpy array.\n",
    "\n",
    "        It is recommended to modify it to suit your needs.\n",
    "\n",
    "        \"\"\"\n",
    "        return observation.to_vect()\n",
    "\n",
    "    def convert_act(self, encoded_act):\n",
    "        \"\"\"\n",
    "        This method will take an \"encoded_act\" (for example a integer) into a valid grid2op action.\n",
    "        \"\"\"\n",
    "        if encoded_act < 0 or encoded_act > len(self.my_action):\n",
    "            raise RuntimeError(\"Invalid action with id {}\".format(encoded_act))\n",
    "        return self.my_actions[encoded_act]\n",
    "\n",
    "    def my_act(self, transformed_observation, reward, done=False):\n",
    "        \"\"\"\n",
    "        This is the main function where you can take your decision.\n",
    "\n",
    "        Instead of:\n",
    "        - calling \"act(observation, reward, done)\" you implement \n",
    "          \"my_act(transformed_observation, reward, done)\"\n",
    "        - this manipulates only \"transformed_observation\" fully flexible as you defined \"convert_obs\"\n",
    "        - and returns \"encoded_action\" that are then digest automatically by \n",
    "          \"convert_act(encoded_act)\" and to return valid actions.\n",
    "\n",
    "        Here we suppose, as many dqn agent, that `my_nn_model` return a vector of size \n",
    "        nb_actions filled with number between 0 and 1 and we take the action given the highest score\n",
    "        \"\"\"\n",
    "        pred_score = self.my_nn_model.predict(transformed_observation, reward, done)\n",
    "        res = np.argmax(pred_score)\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from l2rpn_baselines.utils import BaseDeepQ, TrainingParam\n",
    "import tensorflow.keras as tfk\n",
    "class DuelQ_NN(BaseDeepQ):\n",
    "    \"\"\"Constructs the desired deep q learning network\"\"\"\n",
    "    def __init__(self,\n",
    "                 nn_params,  # neural network meta parameters, defining its architecture\n",
    "                 training_param=None  # training scheme (learning rate, learning rate decay, etc.)\n",
    "                ):\n",
    "        self.action_size = action_size\n",
    "        self.observation_size = observation_size\n",
    "        HIDDEN_FOR_SIMPLICITY\n",
    "\n",
    "    def construct_q_network(self):\n",
    "        \"\"\"\n",
    "        It uses the architecture defined in the `nn_archi` attributes.\n",
    "\n",
    "        \"\"\"\n",
    "        self.model = Sequential()\n",
    "        input_layer = Input(shape=(self.nn_archi.observation_size,),\n",
    "                            name=\"observation\")\n",
    "\n",
    "        lay = input_layer\n",
    "        for lay_num, (size, act) in enumerate(zip(self.nn_archi.sizes, self.nn_archi.activs)):\n",
    "            lay = Dense(size, name=\"layer_{}\".format(lay_num))(lay)  # put at self.action_size\n",
    "            lay = Activation(act)(lay)\n",
    "\n",
    "        fc1 = Dense(self.action_size)(lay)\n",
    "        advantage = Dense(self.action_size, name=\"advantage\")(fc1)\n",
    "\n",
    "        fc2 = Dense(self.action_size)(lay)\n",
    "        value = Dense(1, name=\"value\")(fc2)\n",
    "\n",
    "        meaner = Lambda(lambda x: K.mean(x, axis=1))\n",
    "        mn_ = meaner(advantage)\n",
    "        tmp = subtract([advantage, mn_])\n",
    "        policy = add([tmp, value], name=\"policy\")\n",
    "\n",
    "        self.model = Model(inputs=[input_layer], outputs=[policy])\n",
    "        self.schedule_model, self.optimizer_model = self.make_optimiser()\n",
    "        self.model.compile(loss='mse', optimizer=self.optimizer_model)\n",
    "\n",
    "        self.target_model = Model(inputs=[input_layer], outputs=[policy])\n",
    "    def predict_movement(self, data, epsilon, batch_size=None):\n",
    "        \"\"\"\n",
    "        Predict movement of game controler where is epsilon\n",
    "        probability randomly move.\n",
    "\n",
    "        This was part of the BaseDeepQ and was copy pasted without any modifications\n",
    "        \"\"\"\n",
    "        if batch_size is None:\n",
    "            batch_size = data.shape[0]\n",
    "\n",
    "        rand_val = np.random.random(batch_size)\n",
    "        q_actions = self.model.predict(data, batch_size=batch_size)\n",
    "\n",
    "        opt_policy = np.argmax(np.abs(q_actions), axis=-1)\n",
    "        opt_policy[rand_val < epsilon] = np.random.randint(0, self.action_size, size=(np.sum(rand_val < epsilon)))\n",
    "        return opt_policy, q_actions[0, opt_policy]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from grid2op.Agent import AgentWithConverter  # all converter agent should inherit this\n",
    "from grid2op.Converter import IdToAct  # this is the automatic converter to convert action given as ID (integer)\n",
    "# to valid grid2op action (in particular it is able to compute all actions).\n",
    "\n",
    "from l2rpn_baselines.utils import DeepQAgent\n",
    "from l2rpn_baselines.DoubleDuelingDQN.DoubleDuelingDQN_NN import DoubleDuelingDQN_NN\n",
    "class DuelQSimple(DeepQAgent):\n",
    "    def __init__(self,\n",
    "                 action_space,\n",
    "                 nn_archi,\n",
    "                 HIDDEN_FOR_SIMPLICITY\n",
    "                ):\n",
    "        ...\n",
    "        HIDDEN_FOR_SIMPLICITY\n",
    "        ...\n",
    "        # Load network graph\n",
    "        self.deep_q  = None ## is loaded when first called or when explicitly loaded\n",
    "\n",
    "    # Agent Interface\n",
    "    def convert_obs(self, observation):\n",
    "        \"\"\"\n",
    "        Generic way to convert an observation. This transform it to a vector and the select the attributes\n",
    "        that were selected in :attr:`l2rpn_baselines.utils.NNParams.list_attr_obs` (that have been\n",
    "        extracted once and for all in the :attr:`DeepQAgent._indx_obs` vector).\n",
    "        \"\"\"\n",
    "        obs_as_vect = observation.to_vect()\n",
    "        self._tmp_obs[:] = obs_as_vect[self._indx_obs]\n",
    "        return self._tmp_obs\n",
    "    \n",
    "    def convert_act(self, action):\n",
    "        \"\"\"\n",
    "        calling the convert_act method of the base class.\n",
    "        This is not mandatory as this is the standard behaviour in OOP (object oriented programming)\n",
    "        We only show it here as illustration.\n",
    "        \"\"\"\n",
    "        return super().convert_act(action)\n",
    "\n",
    "    def my_act(self, transformed_observation, reward, done=False):\n",
    "        \"\"\"\n",
    "        This function will return the action (its id) selected by the underlying \n",
    "        :attr:`DeepQAgent.deep_q` network.\n",
    "\n",
    "\n",
    "        Before being used, this method require that the :attr:`DeepQAgent.deep_q` is created. \n",
    "        To that end a call to :func:`DeepQAgent.init_deep_q` needs to have been performed \n",
    "        (this is automatically done if you use baseline we provide and their `evaluate` \n",
    "        and `train` scripts).\n",
    "        \"\"\"\n",
    "        predict_movement_int, *_ = self.deep_q.predict_movement(transformed_observation, epsilon=0.0)\n",
    "        res = int(predict_movement_int)\n",
    "        self._store_action_played(res)\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\virtualenvironment\\L2PRN_grid2op\\lib\\site-packages\\grid2op\\MakeEnv\\Make.py:265: UserWarning: You are using a development environment. This environment is not intended for training agents. It might not be up to date and its primary use if for tests (hence the \"test=True\" you passed as argument). Use at your own risk.\n",
      "  warnings.warn(_MAKE_DEV_ENV_WARN)\n"
     ]
    }
   ],
   "source": [
    "#训练模型\n",
    "# create an environment\n",
    "env = make(env_name,test=True) \n",
    "# don't forget to set \"test=False\" (or remove it, as False is the default value) for \"real\" training\n",
    "\n",
    "# import the train function and train your agent\n",
    "from l2rpn_baselines.DuelQSimple import train\n",
    "from l2rpn_baselines.utils import NNParam, TrainingParam\n",
    "agent_name = \"test_agent\"\n",
    "save_path = \"saved_agent_DDDQN_{}\".format(train_iter)\n",
    "logs_dir=\"tf_logs_DDDQN\"\n",
    "\n",
    "\n",
    "# we then define the neural network we want to make (you may change this at will)\n",
    "## 1. first we choose what \"part\" of the observation we want as input, \n",
    "## here for example only the generator and load information\n",
    "## see https://grid2op.readthedocs.io/en/latest/observation.html#main-observation-attributes\n",
    "## for the detailed about all the observation attributes you want to have\n",
    "li_attr_obs_X = [\"prod_p\", \"prod_v\", \"load_p\", \"load_q\"]\n",
    "# this automatically computes the size of the resulting vector\n",
    "observation_size = NNParam.get_obs_size(env, li_attr_obs_X) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 500/500 [00:17<00:00, 28.34it/s]\n"
     ]
    }
   ],
   "source": [
    "## 2. then we define its architecture\n",
    "sizes = [300, 300, 300]  # 3 hidden layers, of 300 units each, why not...\n",
    "activs =  [\"relu\" for _ in sizes]  # all followed by relu activation, because... why not\n",
    "## 4. you put it all on a dictionnary like that (specific to this baseline)\n",
    "kwargs_archi = {'observation_size': observation_size,\n",
    "                'sizes': sizes,\n",
    "                'activs': activs,\n",
    "                \"list_attr_obs\": li_attr_obs_X}\n",
    "\n",
    "# you can also change the training parameters you are using\n",
    "# more information at https://l2rpn-baselines.readthedocs.io/en/latest/utils.html#l2rpn_baselines.utils.TrainingParam\n",
    "tp = TrainingParam()\n",
    "tp.batch_size = 32  # for example...\n",
    "tp.update_tensorboard_freq = int(train_iter / 10)\n",
    "tp.save_model_each = int(train_iter / 3)\n",
    "tp.min_observation = int(train_iter / 5)\n",
    "train(env,\n",
    "      name=agent_name,\n",
    "      iterations=train_iter,\n",
    "      save_path=save_path,\n",
    "      load_path=None, # put something else if you want to reload an agent instead of creating a new one\n",
    "      logs_dir=logs_dir,\n",
    "      kwargs_archi=kwargs_archi,\n",
    "      training_param=tp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'saved_agent_DDDQN_500'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Environment_rte_case14_redisp' object has no attribute 'get_indx_extract'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-37-35f80374985e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[0mmy_agent\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDuelQSimple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnn_archi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0magent_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[0mmy_agent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msave_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m \u001b[0mmy_agent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minit_obs_extraction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;31m# here we do that to limit the time take, and will only assess the performance on \"max_iter\" iteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\virtualenvironment\\L2PRN_grid2op\\lib\\site-packages\\l2rpn_baselines\\utils\\DeepQAgent.py\u001b[0m in \u001b[0;36minit_obs_extraction\u001b[1;34m(self, observation_space)\u001b[0m\n\u001b[0;32m    289\u001b[0m         \u001b[0mtmp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muint\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# TODO platform independant\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    290\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mobs_attr_name\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_nn_archi\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_obs_attr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 291\u001b[1;33m             \u001b[0mbeg_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mobservation_space\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_indx_extract\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobs_attr_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    292\u001b[0m             \u001b[0mtmp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtmp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbeg_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    293\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_indx_obs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtmp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Environment_rte_case14_redisp' object has no attribute 'get_indx_extract'"
     ]
    }
   ],
   "source": [
    "from grid2op.Runner import Runner\n",
    "from tqdm.notebook import tqdm\n",
    "# chose a scoring function (might be different from the reward you use to train your agent)\n",
    "from grid2op.Reward import L2RPNReward\n",
    "scoring_function = L2RPNReward \n",
    "path_save_results = \"{}_results\".format(save_path)\n",
    "\n",
    "# load your agent (a bit technical to know exactly what to import and how to use it)\n",
    "# this is why we made the \"evaluate\" function that simplifies greatly the process.\n",
    "from l2rpn_baselines.DuelQSimple import DuelQSimple\n",
    "from l2rpn_baselines.DuelQSimple import DuelQ_NNParam\n",
    "path_model, path_target_model = DuelQ_NNParam.get_path_model(save_path, agent_name)\n",
    "nn_archi = DuelQ_NNParam.from_json(os.path.join(path_model, \"nn_architecture.json\"))\n",
    "\n",
    "my_agent = DuelQSimple(env.action_space, nn_archi, name=agent_name)\n",
    "my_agent.load(save_path)\n",
    "my_agent.init_obs_extraction(env)\n",
    "\n",
    "# here we do that to limit the time take, and will only assess the performance on \"max_iter\" iteration\n",
    "dict_params = env.get_params_for_runner()\n",
    "dict_params[\"gridStateclass_kwargs\"][\"max_iter\"] =  max_iter\n",
    "# make a runner from an intialized environment\n",
    "runner = Runner(**dict_params, agentClass=None, agentInstance=my_agent)\n",
    "\n",
    "# run the episode\n",
    "res = runner.run(nb_episode=2, path_save=path_save_results, pbar=tqdm)\n",
    "print(\"The results for the trained agent are:\")\n",
    "for _, chron_name, cum_reward, nb_time_step, max_ts in res:\n",
    "    msg_tmp = \"\\tFor chronics located at {}\\n\".format(chron_name)\n",
    "    msg_tmp += \"\\t\\t - total score: {:.6f}\\n\".format(cum_reward)\n",
    "    msg_tmp += \"\\t\\t - number of time steps completed: {:.0f} / {:.0f}\".format(nb_time_step, max_ts)\n",
    "    print(msg_tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "episode:   0%|                                                                                   | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_65\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "observation (InputLayer)        [(None, 32)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "layer_0 (Dense)                 (None, 300)          9900        observation[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_48 (Activation)      (None, 300)          0           layer_0[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "layer_1 (Dense)                 (None, 300)          90300       activation_48[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_49 (Activation)      (None, 300)          0           layer_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "layer_2 (Dense)                 (None, 300)          90300       activation_49[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_50 (Activation)      (None, 300)          0           layer_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_32 (Dense)                (None, 451)          135751      activation_50[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "advantage (Dense)               (None, 451)          203852      dense_32[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_16 (Lambda)              (None,)              0           advantage[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_33 (Dense)                (None, 451)          135751      activation_50[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "subtract_16 (Subtract)          (None, 451)          0           advantage[0][0]                  \n",
      "                                                                 lambda_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "value (Dense)                   (None, 1)            452         dense_33[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "policy (Add)                    (None, 451)          0           subtract_16[0][0]                \n",
      "                                                                 value[0][0]                      \n",
      "==================================================================================================\n",
      "Total params: 666,306\n",
      "Trainable params: 666,306\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "INFO: \"Sequential runner used.\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "episode:   0%|                                                                                 | 0/100 [00:00<?, ?it/s]\u001b[A\n",
      "episode:   6%|████▍                                                                    | 6/100 [00:00<00:01, 53.72it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: \"Creating path \"C:\\code\\python\\jupyter\\Grid2Op\\saved_agent_DDDQN_500_results\\0\" to save the episode 0\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "episode:  12%|████████▋                                                               | 12/100 [00:00<00:01, 53.00it/s]\u001b[A\n",
      "episode:  17%|████████████▏                                                           | 17/100 [00:00<00:01, 51.15it/s]\u001b[A\n",
      "episode:  23%|████████████████▌                                                       | 23/100 [00:00<00:01, 52.03it/s]\u001b[A\n",
      "episode:  29%|████████████████████▉                                                   | 29/100 [00:00<00:01, 52.48it/s]\u001b[A\n",
      "episode:  34%|████████████████████████▍                                               | 34/100 [00:00<00:01, 51.01it/s]\u001b[A\n",
      "episode:  39%|████████████████████████████                                            | 39/100 [00:00<00:01, 49.84it/s]\u001b[A\n",
      "episode:  45%|████████████████████████████████▍                                       | 45/100 [00:00<00:01, 50.68it/s]\u001b[A\n",
      "episode:  51%|████████████████████████████████████▋                                   | 51/100 [00:00<00:00, 52.09it/s]\u001b[A\n",
      "episode:  57%|█████████████████████████████████████████                               | 57/100 [00:01<00:00, 52.98it/s]\u001b[A\n",
      "episode:  63%|█████████████████████████████████████████████▎                          | 63/100 [00:01<00:00, 52.78it/s]\u001b[A\n",
      "episode:  69%|█████████████████████████████████████████████████▋                      | 69/100 [00:01<00:00, 53.34it/s]\u001b[A\n",
      "episode:  75%|██████████████████████████████████████████████████████                  | 75/100 [00:01<00:00, 54.17it/s]\u001b[A\n",
      "episode:  81%|██████████████████████████████████████████████████████████▎             | 81/100 [00:01<00:00, 53.75it/s]\u001b[A\n",
      "episode:  87%|██████████████████████████████████████████████████████████████▋         | 87/100 [00:01<00:00, 54.17it/s]\u001b[A\n",
      "episode:  93%|██████████████████████████████████████████████████████████████████▉     | 93/100 [00:01<00:00, 54.51it/s]\u001b[A\n",
      "episode: 100%|███████████████████████████████████████████████████████████████████████| 100/100 [00:01<00:00, 52.86it/s]\u001b[A\n",
      "episode:  50%|█████████████████████████████████████▌                                     | 1/2 [00:02<00:02,  2.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: \"Env: 1.62s\n",
      "\t - apply act 0.31s\n",
      "\t - run pf: 1.27s\n",
      "\t - env update + observation: 0.03s\n",
      "Agent: 0.20s\n",
      "Total time: 1.89s\n",
      "Cumulative reward: 121379.789062\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "episode:   0%|                                                                                 | 0/100 [00:00<?, ?it/s]\u001b[A\n",
      "episode:   6%|████▍                                                                    | 6/100 [00:00<00:01, 53.70it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: \"Creating path \"C:\\code\\python\\jupyter\\Grid2Op\\saved_agent_DDDQN_500_results\\1\" to save the episode 1\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "episode:  12%|████████▋                                                               | 12/100 [00:00<00:01, 53.71it/s]\u001b[A\n",
      "episode:  18%|████████████▉                                                           | 18/100 [00:00<00:01, 53.71it/s]\u001b[A\n",
      "episode:  24%|█████████████████▎                                                      | 24/100 [00:00<00:01, 53.57it/s]\u001b[A\n",
      "episode:  29%|████████████████████▉                                                   | 29/100 [00:00<00:01, 51.68it/s]\u001b[A\n",
      "episode:  35%|█████████████████████████▏                                              | 35/100 [00:00<00:01, 52.54it/s]\u001b[A\n",
      "episode:  41%|█████████████████████████████▌                                          | 41/100 [00:00<00:01, 53.45it/s]\u001b[A\n",
      "episode:  47%|█████████████████████████████████▊                                      | 47/100 [00:00<00:00, 54.10it/s]\u001b[A\n",
      "episode:  53%|██████████████████████████████████████▏                                 | 53/100 [00:00<00:00, 53.99it/s]\u001b[A\n",
      "episode:  59%|██████████████████████████████████████████▍                             | 59/100 [00:01<00:00, 53.72it/s]\u001b[A\n",
      "episode:  65%|██████████████████████████████████████████████▊                         | 65/100 [00:01<00:00, 54.30it/s]\u001b[A\n",
      "episode:  71%|███████████████████████████████████████████████████                     | 71/100 [00:01<00:00, 54.86it/s]\u001b[A\n",
      "episode:  77%|███████████████████████████████████████████████████████▍                | 77/100 [00:01<00:00, 55.26it/s]\u001b[A\n",
      "episode:  83%|███████████████████████████████████████████████████████████▊            | 83/100 [00:01<00:00, 55.40it/s]\u001b[A\n",
      "episode:  89%|████████████████████████████████████████████████████████████████        | 89/100 [00:01<00:00, 55.18it/s]\u001b[A\n",
      "episode: 100%|███████████████████████████████████████████████████████████████████████| 100/100 [00:01<00:00, 54.33it/s]\u001b[A\n",
      "episode: 100%|███████████████████████████████████████████████████████████████████████████| 2/2 [00:04<00:00,  2.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: \"Env: 1.59s\n",
      "\t - apply act 0.31s\n",
      "\t - run pf: 1.25s\n",
      "\t - env update + observation: 0.04s\n",
      "Agent: 0.19s\n",
      "Total time: 1.84s\n",
      "Cumulative reward: 123131.984375\"\n",
      "Evaluation summary:\n",
      "chronics at: 0\ttotal score: 121379.789062\ttime steps: 100/100\n",
      "chronics at: 1\ttotal score: 123131.984375\ttime steps: 100/100\n",
      "The agent played 1 different action\n",
      "Action with ID 118 was played 200 times\n",
      "This action will:\n",
      "\t - NOT change anything to the injections\n",
      "\t - NOT perform any redispatching action\n",
      "\t - NOT force any line status\n",
      "\t - switch status of 1 powerlines ([17])\n",
      "\t - NOT switch anything in the topology\n",
      "\t - NOT force any particular bus configuration\n",
      "-----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from l2rpn_baselines.DuelQSimple import evaluate\n",
    "path_save_results = \"{}_results\".format(save_path)\n",
    "\n",
    "evaluated_agent, res_runner = evaluate(env,\n",
    "                                       name=agent_name,\n",
    "                                       load_path=save_path,\n",
    "                                       logs_path=path_save_results,\n",
    "                                       nb_episode=2,\n",
    "                                       nb_process=1,\n",
    "                                       max_steps=100,\n",
    "                                       verbose=True,\n",
    "                                       save_gif=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Environment_rte_case14_redisp' object has no attribute 'get_indx_extract'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-25-0a4085f36d64>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmy_agent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minit_obs_extraction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# here we do that to limit the time take, and will only assess the performance on \"max_iter\" iteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mdict_params\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_params_for_runner\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mdict_params\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"gridStateclass_kwargs\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"max_iter\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m  \u001b[0mmax_iter\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\virtualenvironment\\L2PRN_grid2op\\lib\\site-packages\\l2rpn_baselines\\utils\\DeepQAgent.py\u001b[0m in \u001b[0;36minit_obs_extraction\u001b[1;34m(self, observation_space)\u001b[0m\n\u001b[0;32m    289\u001b[0m         \u001b[0mtmp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muint\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# TODO platform independant\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    290\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mobs_attr_name\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_nn_archi\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_obs_attr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 291\u001b[1;33m             \u001b[0mbeg_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mobservation_space\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_indx_extract\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobs_attr_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    292\u001b[0m             \u001b[0mtmp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtmp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbeg_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    293\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_indx_obs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtmp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Environment_rte_case14_redisp' object has no attribute 'get_indx_extract'"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:L2PRN_grid2op]",
   "language": "python",
   "name": "conda-env-L2PRN_grid2op-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
